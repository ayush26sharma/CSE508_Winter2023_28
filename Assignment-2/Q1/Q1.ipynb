{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e66935bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8d2b4cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Ayush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Ayush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e13dd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rem_sw(arr):\n",
    "  global stop_words\n",
    "  reg_str = r'^'\n",
    "  for i in range(len(arr)):\n",
    "    for sw in stop_words:\n",
    "      reg_str+=sw\n",
    "      reg_str+='$'\n",
    "      arr[i] = re.sub(reg_str, '', arr[i])\n",
    "      reg_str = r'^'\n",
    "  return arr\n",
    "\n",
    "def rem_punc(str):\n",
    "  str = re.sub(r'(\\.|\\?|\\,|!|\\:|\\;|\\&|\\-|\\(|\\)|\\{|\\}|\\'|\\\"|\\/)', ' ', str)\n",
    "  return str\n",
    "\n",
    "def preprocessing(s):\n",
    "    s = s.lower()\n",
    "    s = re.sub(r'(\\.|\\?|\\,|!|\\:|\\;|\\&|\\-|\\(|\\)|\\{|\\}|\\'|\\\"|\\/)', ' ', s)\n",
    "    s = word_tokenize(s)\n",
    "    s = rem_sw(s)\n",
    "    for j in range(len(s)):\n",
    "      s[j] = rem_punc(s[j])\n",
    "    s[:] = [j for j in s if (j!='' and j!=' ')]\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b5589c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tf(token_list):\n",
    "    tf_dict = {}\n",
    "    for token in token_list:\n",
    "        tf_dict[token] = tf_dict.get(token, 0) + 1\n",
    "    return tf_dict\n",
    "\n",
    "def compute_df(documents):\n",
    "    df_dict = {}\n",
    "    for doc in documents:\n",
    "        for token in set(documents[doc]):\n",
    "            df_dict[token] = df_dict.get(token, 0) + 1\n",
    "    return df_dict\n",
    "\n",
    "def compute_idf(documents):\n",
    "    idf_dict = {}\n",
    "    N = len(documents)\n",
    "    df = compute_df(documents)\n",
    "    for term in df:\n",
    "        idf_dict[term] = math.log((N+1)/(df[term]+1))\n",
    "    return idf_dict\n",
    "\n",
    "def compute_tf_idf(documents):\n",
    "    tf_idf_matrix = {}\n",
    "    idf = compute_idf(documents)\n",
    "    for doc in documents:\n",
    "        tf = compute_tf(documents[doc])\n",
    "        tf_idf_matrix[doc] = {}\n",
    "        for term in tf:\n",
    "            tf_idf_matrix[doc][term] = tf[term] * idf[term]\n",
    "    return tf_idf_matrix\n",
    "\n",
    "def compute_query_tf_idf(query, idf):\n",
    "    query_tf = compute_tf(query)\n",
    "    query_tf_idf = {}\n",
    "    for term in query_tf:\n",
    "        if term in idf:\n",
    "            query_tf_idf[term] = query_tf[term] * idf[term]\n",
    "    return query_tf_idf\n",
    "\n",
    "def compute_cosine_similarity(doc_vector, query_vector):\n",
    "    dot_product = 0\n",
    "    for term in query_vector:\n",
    "        if term in doc_vector:\n",
    "            dot_product += doc_vector[term] * query_vector[term]\n",
    "    doc_norm = math.sqrt(sum([i**2 for i in doc_vector.values()]))\n",
    "    query_norm = math.sqrt(sum([i**2 for i in query_vector.values()]))\n",
    "    if doc_norm == 0 or query_norm == 0:\n",
    "        return 0\n",
    "    return dot_product / (doc_norm * query_norm)\n",
    "\n",
    "def rank_documents(documents, query, tf_weighting):\n",
    "    tf_idf_matrix = compute_tf_idf(documents)\n",
    "    idf = compute_idf(documents)\n",
    "    query_tf_idf = compute_query_tf_idf(query, idf)\n",
    "    rankings = {}\n",
    "    for doc in tf_idf_matrix:\n",
    "        doc_vector = tf_idf_matrix[doc]\n",
    "        if tf_weighting == 'binary':\n",
    "            for term in doc_vector:\n",
    "                if doc_vector[term] > 0:\n",
    "                    doc_vector[term] = 1\n",
    "        elif tf_weighting == 'raw_count':\n",
    "            pass\n",
    "        elif tf_weighting == 'term_frequency':\n",
    "            for term in doc_vector:\n",
    "                doc_vector[term] = doc_vector[term] / compute_tf(documents[doc])[term]\n",
    "        elif tf_weighting == 'log_normalization':\n",
    "            for term in doc_vector:\n",
    "                doc_vector[term] = math.log(1 + doc_vector[term])\n",
    "        elif tf_weighting == 'double_normalization':\n",
    "            max_tf = max(doc_vector.values())\n",
    "            for term in doc_vector:\n",
    "                doc_vector[term] = 0.5 + 0.5 * (doc_vector[term] / max_tf)\n",
    "        else:\n",
    "            raise ValueError('Invalid TF weighting scheme')\n",
    "        similarity = compute_cosine_similarity(doc_vector, query_tf_idf)\n",
    "        rankings[doc] = similarity\n",
    "        \n",
    "    sorted_rankings = sorted(rankings.items(), key=lambda x: x[1], reverse=True)\n",
    "    return sorted_rankings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "74c5e42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_driver(documents, query):\n",
    "    \n",
    "    print('\\n------------------------------------------------------------------------')\n",
    "    print('Binary weighting scheme:')\n",
    "    binary_rankings = rank_documents(documents, query, 'binary')\n",
    "    for doc, score in binary_rankings[:5]:\n",
    "        print(f'{doc}: {score}')\n",
    "    print('\\n------------------------------------------------------------------------')\n",
    "    \n",
    "    print('\\nRaw count weighting scheme:')\n",
    "    raw_count_rankings = rank_documents(documents, query, 'raw_count')\n",
    "    for doc, score in raw_count_rankings[:5]:\n",
    "        print(f'{doc}: {score}')\n",
    "    print('\\n------------------------------------------------------------------------')\n",
    "    \n",
    "    print('\\nTerm frequency weighting scheme:')\n",
    "    term_frequency_rankings = rank_documents(documents, query, 'term_frequency')\n",
    "    for doc, score in term_frequency_rankings[:5]:\n",
    "        print(f'{doc}: {score}')\n",
    "    print('\\n------------------------------------------------------------------------')\n",
    "    \n",
    "    print('\\nLog normalization weighting scheme:')\n",
    "    log_normalization_rankings = rank_documents(documents, query, 'log_normalization')\n",
    "    for doc, score in log_normalization_rankings[:5]:\n",
    "        print(f'{doc}: {score}')\n",
    "    print('\\n------------------------------------------------------------------------')\n",
    "    \n",
    "    print('\\nDouble normalization weighting scheme:')\n",
    "    double_normalization_rankings = rank_documents(documents, query, 'double_normalization')\n",
    "    for doc, score in double_normalization_rankings[:5]:\n",
    "        print(f'{doc}: {score}')\n",
    "    print('\\n------------------------------------------------------------------------')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "93897753",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard(tokenised_docs, query):\n",
    "    jaccard_scores = {}\n",
    "    \n",
    "    for doc, tokens in tokenised_docs.items():\n",
    "        tokens = set(tokens)\n",
    "        intersection = tokens.intersection(query)\n",
    "        union = tokens.union(query)\n",
    "        jaccard_scores[doc] = len(intersection) / len(union)\n",
    "\n",
    "    sorted_docs = sorted(jaccard_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    i = 0\n",
    "    print(\"S.No.||   Document Name   ||   Jaccard Coefficient\")\n",
    "    print(\"----------------------------------------------------\")\n",
    "    for doc_name, score in sorted_docs[:10]:\n",
    "        print(f\"  {i}  ||   {doc_name}   ||   {score}\")\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b73b1386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your raw query: experimental is the average\n",
      "\n",
      "Top 10 documents with highest Jaccard Coefficient are: \n",
      "\n",
      "S.No.||   Document Name   ||   Jaccard Coefficient\n",
      "----------------------------------------------------\n",
      "  0  ||   cranfield1045   ||   0.07142857142857142\n",
      "  1  ||   cranfield0137   ||   0.045454545454545456\n",
      "  2  ||   cranfield0286   ||   0.045454545454545456\n",
      "  3  ||   cranfield0074   ||   0.044444444444444446\n",
      "  4  ||   cranfield0271   ||   0.043478260869565216\n",
      "  5  ||   cranfield1146   ||   0.043478260869565216\n",
      "  6  ||   cranfield0670   ||   0.04\n",
      "  7  ||   cranfield0339   ||   0.03571428571428571\n",
      "  8  ||   cranfield0932   ||   0.034482758620689655\n",
      "  9  ||   cranfield0501   ||   0.03225806451612903\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "Binary weighting scheme:\n",
      "cranfield0074: 0.19384860436389673\n",
      "cranfield0400: 0.16156789118984793\n",
      "cranfield0959: 0.15542465623585575\n",
      "cranfield0497: 0.14916335374445833\n",
      "cranfield0741: 0.14321462866692672\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "Raw count weighting scheme:\n",
      "cranfield0741: 0.23538607058853622\n",
      "cranfield1012: 0.18315712592222966\n",
      "cranfield0074: 0.17401750053978673\n",
      "cranfield0604: 0.16965340996384137\n",
      "cranfield0822: 0.13694192732232213\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "Term frequency weighting scheme:\n",
      "cranfield0074: 0.19854041053982044\n",
      "cranfield0568: 0.17723980194005573\n",
      "cranfield0400: 0.17281111227516532\n",
      "cranfield0959: 0.15699173573683597\n",
      "cranfield0741: 0.1484694205991057\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "Log normalization weighting scheme:\n",
      "cranfield0074: 0.1936174090592164\n",
      "cranfield0741: 0.18907084817209432\n",
      "cranfield1012: 0.16373274993401607\n",
      "cranfield0400: 0.16035839757150075\n",
      "cranfield0604: 0.1582255131887565\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "Double normalization weighting scheme:\n",
      "cranfield0074: 0.19237644667886908\n",
      "cranfield0741: 0.17537067314773924\n",
      "cranfield0400: 0.15761561991156842\n",
      "cranfield0959: 0.1474858697691769\n",
      "cranfield1012: 0.14171995196025589\n",
      "\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    with open('tokens.pickle', \"rb\") as f:\n",
    "        tokenised_docs = pickle.load(f)\n",
    "    \n",
    "    raw_query = input(\"Enter your raw query: \")\n",
    "    query =  preprocessing(raw_query)\n",
    "\n",
    "    print('\\nTop 10 documents with highest Jaccard Coefficient are: \\n')    \n",
    "    jaccard(tokenised_docs, query)\n",
    "    tfidf_driver(tokenised_docs, query)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
