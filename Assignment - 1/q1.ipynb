{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\rupin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rupin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "import pickle5 as pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'CSE508_Winter2023_Dataset/CSE508_Winter2023_Dataset/'\n",
    "PATH_upd = 'CSE508_Winter2023_Dataset_upd/CSE508_Winter2023_Dataset/'\n",
    "flist = os.listdir(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in flist:\n",
    "    s= open(PATH+i).read()\n",
    "    ans = s[s.index('<TITLE>')+8:s.index('</TITLE>')].strip() + \" \" + s[s.index('<TEXT>')+7:s.index('</TEXT>')].strip()\n",
    "    open(PATH_upd+i, 'w').write(ans)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rem_sw(arr):\n",
    "  global stop_words\n",
    "  reg_str = r'^'\n",
    "  for i in range(len(arr)):\n",
    "    for sw in stop_words:\n",
    "      reg_str+=sw\n",
    "      reg_str+='$'\n",
    "      arr[i] = re.sub(reg_str, '', arr[i])\n",
    "      reg_str = r'^'\n",
    "  return arr\n",
    "\n",
    "def rem_punc(str):\n",
    "  str = re.sub(r'(\\.|\\?|\\,|!|\\:|\\;|\\&|\\-|\\(|\\)|\\{|\\}|\\'|\\\"|\\/)', ' ', str)\n",
    "  return str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in flist:\n",
    "    s = open(PATH_upd+i, 'r').read()\n",
    "    s = s.lower()\n",
    "    open(PATH_upd+i, 'w').write(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(s):\n",
    "    s = s.lower()\n",
    "    s = re.sub(r'(\\.|\\?|\\,|!|\\:|\\;|\\&|\\-|\\(|\\)|\\{|\\}|\\'|\\\"|\\/)', ' ', s)\n",
    "    s = word_tokenize(s)\n",
    "    s = rem_sw(s)\n",
    "    for j in range(len(s)):\n",
    "      s[j] = rem_punc(s[j])\n",
    "    s[:] = [j for j in s if (j!='' and j!=' ')]\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in flist:\n",
    "    s = open(PATH_upd+i, 'r').read()\n",
    "    s = s.lower()\n",
    "    s = rem_punc(s)\n",
    "    s = word_tokenize(s)\n",
    "    s = rem_sw(s)\n",
    "    for j in range(len(s)):\n",
    "      s[j] = rem_punc(s[j])\n",
    "    s[:] = [j for j in s if (j!='' and j!=' ')]\n",
    "    open(PATH_upd+i, 'w').write(','.join(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos_index structure: {term : [df, {doc_id : [tf, [pos1, pos2, ...]]}]} df = frequency in all docs combined, tf = frequency in a particular doc, pos[] = positions of term in doc\n",
    "pos_index = {}\n",
    "for doc_id in flist:\n",
    "    f = open(PATH_upd+doc_id, 'r')\n",
    "    tokens = f.read().split(',')\n",
    "    for num,j in enumerate(tokens):\n",
    "        if j not in set(pos_index.keys()):  #new term\n",
    "            pos_index[j] = []\n",
    "            pos_index[j].append(1)\n",
    "            pos_index[j].append({doc_id:[1,[num]]})\n",
    "        else:\n",
    "            pos_index[j][0]+=1\n",
    "            if doc_id not in set(pos_index[j][1].keys()):   #new document for this term\n",
    "                pos_index[j][1][doc_id] = [1]\n",
    "                pos_index[j][1][doc_id].append([num])\n",
    "            else:\n",
    "                pos_index[j][1][doc_id][1].append(num)\n",
    "                pos_index[j][1][doc_id][0]+=1\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pos_index.pickle', 'wb') as f:\n",
    "    pickle.dump(pos_index, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('pos_index.pickle', 'rb') as f:\n",
    "    pos_index = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = input()\n",
    "query = preprocessing(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['experimental', 'investigation']\n"
     ]
    }
   ],
   "source": [
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['investigation', 'experimental']\n",
      "[423, 281]\n"
     ]
    }
   ],
   "source": [
    "term_freq = []\n",
    "for term in query:\n",
    "    if term in set(pos_index.keys()):\n",
    "        term_freq.append(pos_index[term][0])\n",
    "    else:\n",
    "        term_freq.append(0)\n",
    "si = [sorted_index[0] for sorted_index in sorted(enumerate(term_freq),key=lambda i:i[1])]\n",
    "sorted_query = [query[i] for i in si]\n",
    "print(sorted_query)\n",
    "print(term_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "216\n"
     ]
    }
   ],
   "source": [
    "common_docs = list(pos_index[sorted_query[0]][1].keys())\n",
    "for term in range(1,len(sorted_query)):\n",
    "    if term in set(pos_index.keys()):\n",
    "        common_docs = [i for i in common_docs if i in set(pos_index[term][1].keys())]\n",
    "print(len(common_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['experimental', 'investigation']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans_docs = {}   # stores the possible documents with list of possible phrase start positions\n",
    "for n, term in enumerate(query):\n",
    "    if n==0:    # for the first term\n",
    "        #ans_docs = pos_index[term][1]\n",
    "        for i in list(set(pos_index[term][1].keys())):\n",
    "            ans_docs[i] = pos_index[term][1][i][1].copy()\n",
    "        #print(ans_docs['cranfield1201'])\n",
    "        #for i in list(set(pos_index[term][1].keys())):\n",
    "            #ans_docs[i] = pos_index[term][1][i]\n",
    "    else:\n",
    "        to_check_docs = list(set(pos_index[term][1].keys()).intersection(set(ans_docs.keys())))\n",
    "        #print(to_check_docs)\n",
    "        for doc in to_check_docs:\n",
    "            temp = []\n",
    "            for pos in pos_index[term][1][doc][1]:\n",
    "                if doc=='cranfield1201':\n",
    "                    #print(pos-n)\n",
    "                    pass\n",
    "                if pos-n in set(ans_docs[doc]):\n",
    "                    \n",
    "                    temp.append(pos-n)\n",
    "                    #ans_docs[doc].remove(pos-n)\n",
    "            \"\"\"for i in ans_docs[doc]:\n",
    "                if doc=='cranfield1201':\n",
    "                    print(i)\n",
    "                    print(ans_docs['cranfield1201'])\n",
    "                if i not in set(temp):\n",
    "                    ans_docs[doc].remove(i)\"\"\"\n",
    "            ans_docs[doc] = [i for i in ans_docs[doc] if i in set(temp)]\n",
    "                \n",
    "            if len(ans_docs[doc]) ==0:\n",
    "                del ans_docs[doc]\n",
    "        for doc in list(set(ans_docs.keys()).difference(set(to_check_docs))):\n",
    "            del ans_docs[doc]\n",
    "        if len(ans_docs) == 0:\n",
    "            print(\"No documents found\")\n",
    "            break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "print(len(list(ans_docs.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, [2, 14, 34, 46, 89, 115]]"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_index['flow'][1]['cranfield0002']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7065473ddf06590dfe845d506c9721c9c7a6baf42eb3fc876ce07c8f7fc6e906"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
