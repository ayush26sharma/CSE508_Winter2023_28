{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vdt-D5eNi3gw",
        "outputId": "d3a657d4-caac-4a9c-f4b6-cfabd9e20d29"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\rupin\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\rupin\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import glob\n",
        "import os\n",
        "import re\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "import pickle5 as pickle\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "oiolrvEBi3g2"
      },
      "outputs": [],
      "source": [
        "PATH = 'CSE508_Winter2023_Dataset/CSE508_Winter2023_Dataset'\n",
        "PATH_upd = 'CSE508_Winter2023_Dataset_upd/CSE508_Winter2023_Dataset'\n",
        "flist = os.listdir(PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "BVi3iHbTi3g4"
      },
      "outputs": [],
      "source": [
        "def rem_sw(arr):\n",
        "  global stop_words\n",
        "  reg_str = r'^'\n",
        "  for i in range(len(arr)):\n",
        "    for sw in stop_words:\n",
        "      reg_str+=sw\n",
        "      reg_str+='$'\n",
        "      arr[i] = re.sub(reg_str, '', arr[i])\n",
        "      reg_str = r'^'\n",
        "  return arr\n",
        "\n",
        "def rem_punc(str):\n",
        "  str = re.sub(r'(\\.|\\?|\\,|!|\\:|\\;|\\&|\\-|\\(|\\)|\\{|\\}|\\'|\\\"|\\/)', ' ', str)\n",
        "  return str"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "cvpveTdiu4rJ"
      },
      "outputs": [],
      "source": [
        "### biword index creation ###\n",
        "biword_index_dict = {}\n",
        "def bigram_inv_index(file_data, file_id):\n",
        "        file_data_list = file_data.split(\",\")\n",
        "        # print(file_data_list)\n",
        "        i=0\n",
        "        while(i<len(file_data_list)-1):\n",
        "          biword_term = file_data_list[i]+\" \"+file_data_list[i+1]\n",
        "\n",
        "          if biword_term not in biword_index_dict:\n",
        "            biword_index_dict[biword_term] = []\n",
        "            biword_index_dict[biword_term].append(file_id)\n",
        "\n",
        "          elif file_id in biword_index_dict[biword_term]:\n",
        "            pass\n",
        "          \n",
        "          else:\n",
        "            biword_index_dict[biword_term].append(file_id)\n",
        "\n",
        "          i+=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "zzIdRpnphiv9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "directory = PATH_upd\n",
        "file_no = 0\n",
        "all_files = {}           # stores the (file names : file_no )\n",
        "for filename in os.listdir(directory):\n",
        "    f = os.path.join(directory, filename)\n",
        "    if os.path.isfile(f):\n",
        "\n",
        "        file = open(f,errors=\"ignore\") \n",
        "        bigram_inv_index(file.read(),file_no)\n",
        "        all_files[file_no] = filename\n",
        "        file_no += 1\n",
        "\n",
        "        file.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# pos_index structure: {term : [df, {doc_id : [tf, [pos1, pos2, ...]]}]} df = frequency in all docs combined, tf = frequency in a particular doc, pos[] = positions of term in doc\n",
        "pos_index = {}\n",
        "for doc_id in flist:\n",
        "    f = open(PATH_upd+doc_id, 'r')\n",
        "    tokens = f.read().split(',')\n",
        "    for num,j in enumerate(tokens):\n",
        "        if j not in set(pos_index.keys()):  #new term\n",
        "            pos_index[j] = []\n",
        "            pos_index[j].append(1)\n",
        "            pos_index[j].append({doc_id:[1,[num]]})\n",
        "        else:\n",
        "            pos_index[j][0]+=1\n",
        "            if doc_id not in set(pos_index[j][1].keys()):   #new document for this term\n",
        "                pos_index[j][1][doc_id] = [1]\n",
        "                pos_index[j][1][doc_id].append([num])\n",
        "            else:\n",
        "                pos_index[j][1][doc_id][1].append(num)\n",
        "                pos_index[j][1][doc_id][0]+=1\n",
        "    f.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open('pos_index.pickle', 'wb') as f:\n",
        "    pickle.dump(pos_index, f, protocol=pickle.HIGHEST_PROTOCOL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open('pos_index.pickle', 'rb') as f:\n",
        "    pos_index = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'cranfield0001'"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "all_files[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def eval_pos_query(query):\n",
        "    ans_docs = {}   # stores the possible documents with list of possible phrase start positions\n",
        "    for n, term in enumerate(query):\n",
        "        if n==0:    # for the first term\n",
        "            #ans_docs = pos_index[term][1]\n",
        "            for i in list(set(pos_index[term][1].keys())):\n",
        "                ans_docs[i] = pos_index[term][1][i][1].copy()\n",
        "            #print(ans_docs['cranfield1201'])\n",
        "            #for i in list(set(pos_index[term][1].keys())):\n",
        "                #ans_docs[i] = pos_index[term][1][i]\n",
        "        else:\n",
        "            to_check_docs = list(set(pos_index[term][1].keys()).intersection(set(ans_docs.keys())))\n",
        "            #print(to_check_docs)\n",
        "            for doc in to_check_docs:\n",
        "                temp = []\n",
        "                for pos in pos_index[term][1][doc][1]:\n",
        "                    if pos-n in set(ans_docs[doc]):\n",
        "                        temp.append(pos-n)\n",
        "                        #ans_docs[doc].remove(pos-n)\n",
        "                ans_docs[doc] = [i for i in ans_docs[doc] if i in set(temp)]\n",
        "                    \n",
        "                if len(ans_docs[doc]) ==0:  # if no possible phrase start positions left then remove the document\n",
        "                    del ans_docs[doc]\n",
        "            for doc in list(set(ans_docs.keys()).difference(set(to_check_docs))): # remove documents which do not contain the term\n",
        "                del ans_docs[doc]\n",
        "            if len(ans_docs) == 0:\n",
        "                return ans_docs\n",
        "    return ans_docs\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "SsJbnD3b29fI"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "with open('biword_dict.pickle', 'wb') as f:\n",
        "    pickle.dump(biword_index_dict, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vWx86BCp2EST",
        "outputId": "f119b878-8447-442f-c47a-5e406bcfab11"
      },
      "outputs": [],
      "source": [
        "with open('biword_dict.pickle', 'rb') as f:\n",
        "    biword_index_dict = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "5siylEWDyht7"
      },
      "outputs": [],
      "source": [
        "def intersection(list1, list2):\n",
        "    list3=[]\n",
        "    for value in list1:\n",
        "      if(value in set(list2)):\n",
        "        list3.append(value)\n",
        "    \n",
        "    return list3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "98clG6RvsO_p"
      },
      "outputs": [],
      "source": [
        "def queryPreprocess(s):\n",
        "  s = s.lower()\n",
        "  s = re.sub(r'(\\.|\\?|\\,|!|\\:|\\;|\\&|\\-|\\(|\\)|\\{|\\}|\\'|\\\"|\\/)', ' ', s)\n",
        "  s = word_tokenize(s)\n",
        "  s = rem_sw(s)\n",
        "  for j in range(len(s)):\n",
        "    s[j] = rem_punc(s[j])\n",
        "  s[:] = [j for j in s if (j!='' and j!=' ')]\n",
        "  return s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JoBWUR6Pug2Z",
        "outputId": "797b253d-968e-4eec-8fd0-0d851bc9ca65"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['experimental']\n",
            "Number of documents retrieved for query 1400 using bigram inverted index: 0\n",
            "Names of documents retrieved for query 1400 using bigram inverted index: []\n",
            "Number of documents retrieved for query 1400 using positional index: 318\n",
            "Names of documents retrieved for query 1400 using positional index: ['cranfield0001', 'cranfield0011', 'cranfield0012', 'cranfield0017', 'cranfield0019', 'cranfield0025', 'cranfield0029', 'cranfield0030', 'cranfield0035', 'cranfield0041', 'cranfield0042', 'cranfield0047', 'cranfield0052', 'cranfield0053', 'cranfield0058', 'cranfield0069', 'cranfield0070', 'cranfield0074', 'cranfield0078', 'cranfield0084', 'cranfield0099', 'cranfield0101', 'cranfield0103', 'cranfield0112', 'cranfield0115', 'cranfield0121', 'cranfield0123', 'cranfield0137', 'cranfield0140', 'cranfield0142', 'cranfield0154', 'cranfield0156', 'cranfield0168', 'cranfield0170', 'cranfield0171', 'cranfield0173', 'cranfield0176', 'cranfield0179', 'cranfield0183', 'cranfield0184', 'cranfield0186', 'cranfield0187', 'cranfield0188', 'cranfield0189', 'cranfield0191', 'cranfield0195', 'cranfield0197', 'cranfield0202', 'cranfield0203', 'cranfield0206', 'cranfield0207', 'cranfield0212', 'cranfield0216', 'cranfield0220', 'cranfield0222', 'cranfield0225', 'cranfield0227', 'cranfield0230', 'cranfield0234', 'cranfield0245', 'cranfield0251', 'cranfield0256', 'cranfield0257', 'cranfield0262', 'cranfield0271', 'cranfield0273', 'cranfield0277', 'cranfield0282', 'cranfield0283', 'cranfield0286', 'cranfield0294', 'cranfield0295', 'cranfield0304', 'cranfield0307', 'cranfield0329', 'cranfield0330', 'cranfield0334', 'cranfield0338', 'cranfield0339', 'cranfield0344', 'cranfield0345', 'cranfield0346', 'cranfield0347', 'cranfield0354', 'cranfield0360', 'cranfield0369', 'cranfield0370', 'cranfield0372', 'cranfield0377', 'cranfield0397', 'cranfield0409', 'cranfield0411', 'cranfield0413', 'cranfield0418', 'cranfield0420', 'cranfield0421', 'cranfield0423', 'cranfield0427', 'cranfield0435', 'cranfield0439', 'cranfield0441', 'cranfield0442', 'cranfield0453', 'cranfield0455', 'cranfield0462', 'cranfield0464', 'cranfield0467', 'cranfield0484', 'cranfield0494', 'cranfield0496', 'cranfield0497', 'cranfield0498', 'cranfield0501', 'cranfield0503', 'cranfield0504', 'cranfield0505', 'cranfield0511', 'cranfield0518', 'cranfield0520', 'cranfield0522', 'cranfield0536', 'cranfield0540', 'cranfield0544', 'cranfield0549', 'cranfield0552', 'cranfield0553', 'cranfield0558', 'cranfield0563', 'cranfield0567', 'cranfield0569', 'cranfield0572', 'cranfield0576', 'cranfield0588', 'cranfield0595', 'cranfield0600', 'cranfield0606', 'cranfield0610', 'cranfield0632', 'cranfield0634', 'cranfield0635', 'cranfield0636', 'cranfield0644', 'cranfield0645', 'cranfield0649', 'cranfield0662', 'cranfield0663', 'cranfield0666', 'cranfield0670', 'cranfield0675', 'cranfield0678', 'cranfield0679', 'cranfield0685', 'cranfield0688', 'cranfield0689', 'cranfield0694', 'cranfield0704', 'cranfield0712', 'cranfield0713', 'cranfield0717', 'cranfield0720', 'cranfield0728', 'cranfield0729', 'cranfield0739', 'cranfield0740', 'cranfield0743', 'cranfield0753', 'cranfield0760', 'cranfield0766', 'cranfield0767', 'cranfield0772', 'cranfield0781', 'cranfield0790', 'cranfield0801', 'cranfield0802', 'cranfield0806', 'cranfield0816', 'cranfield0820', 'cranfield0823', 'cranfield0825', 'cranfield0827', 'cranfield0829', 'cranfield0830', 'cranfield0836', 'cranfield0844', 'cranfield0845', 'cranfield0846', 'cranfield0847', 'cranfield0856', 'cranfield0857', 'cranfield0858', 'cranfield0863', 'cranfield0866', 'cranfield0867', 'cranfield0869', 'cranfield0878', 'cranfield0881', 'cranfield0887', 'cranfield0891', 'cranfield0907', 'cranfield0911', 'cranfield0912', 'cranfield0923', 'cranfield0924', 'cranfield0927', 'cranfield0928', 'cranfield0932', 'cranfield0935', 'cranfield0946', 'cranfield0950', 'cranfield0951', 'cranfield0954', 'cranfield0955', 'cranfield0959', 'cranfield0961', 'cranfield0964', 'cranfield0965', 'cranfield0974', 'cranfield0984', 'cranfield0986', 'cranfield0996', 'cranfield0997', 'cranfield0999', 'cranfield1006', 'cranfield1008', 'cranfield1016', 'cranfield1019', 'cranfield1028', 'cranfield1039', 'cranfield1040', 'cranfield1045', 'cranfield1046', 'cranfield1049', 'cranfield1051', 'cranfield1062', 'cranfield1066', 'cranfield1069', 'cranfield1070', 'cranfield1074', 'cranfield1075', 'cranfield1076', 'cranfield1078', 'cranfield1080', 'cranfield1081', 'cranfield1082', 'cranfield1083', 'cranfield1092', 'cranfield1097', 'cranfield1098', 'cranfield1112', 'cranfield1118', 'cranfield1122', 'cranfield1125', 'cranfield1127', 'cranfield1145', 'cranfield1146', 'cranfield1151', 'cranfield1153', 'cranfield1155', 'cranfield1156', 'cranfield1158', 'cranfield1159', 'cranfield1160', 'cranfield1161', 'cranfield1167', 'cranfield1171', 'cranfield1177', 'cranfield1185', 'cranfield1186', 'cranfield1187', 'cranfield1192', 'cranfield1195', 'cranfield1196', 'cranfield1198', 'cranfield1199', 'cranfield1204', 'cranfield1205', 'cranfield1209', 'cranfield1212', 'cranfield1213', 'cranfield1214', 'cranfield1216', 'cranfield1218', 'cranfield1220', 'cranfield1222', 'cranfield1225', 'cranfield1227', 'cranfield1228', 'cranfield1230', 'cranfield1231', 'cranfield1234', 'cranfield1237', 'cranfield1261', 'cranfield1262', 'cranfield1263', 'cranfield1264', 'cranfield1268', 'cranfield1269', 'cranfield1277', 'cranfield1290', 'cranfield1298', 'cranfield1302', 'cranfield1310', 'cranfield1314', 'cranfield1337', 'cranfield1338', 'cranfield1339', 'cranfield1341', 'cranfield1352', 'cranfield1363', 'cranfield1364', 'cranfield1369', 'cranfield1372', 'cranfield1374', 'cranfield1384', 'cranfield1390', 'cranfield1392', 'cranfield1396', 'cranfield1397']\n"
          ]
        }
      ],
      "source": [
        "n=int(input(\"Enter number of queries: \"))\n",
        "for i in range(n):\n",
        "  q=input(\"Enter query\")\n",
        "  query = queryPreprocess(q)  #returns tokenized processed query\n",
        "  print(query)\n",
        "  pos_ans = eval_pos_query(query)\n",
        "  list_=[]\n",
        "  for j in range(len(query)):\n",
        "    if(query[j]!=''):\n",
        "      list_.append(query[j])\n",
        "  biwords=[]\n",
        "  for j in range(len(list_)-1):\n",
        "    biwords.append(list_[j]+\" \"+list_[j+1])\n",
        "\n",
        "\n",
        "  ans=[]\n",
        "  for i in range(1400):# initial list of all files\n",
        "    ans.append(i)\n",
        "\n",
        "  for i in range(len(biwords)):\n",
        "    term=biwords[i]\n",
        "    if(term in biword_index_dict.keys()):\n",
        "      ans=intersection(ans,biword_index_dict[term])\n",
        "    else:\n",
        "      ans=[]\n",
        "      break\n",
        "  for j in range(len(ans)):\n",
        "    ans[j]=all_files[ans[j]]\n",
        "  if len(query)==1:\n",
        "    ans = []\n",
        "  \n",
        "  ans.sort()\n",
        "  print(f\"Number of documents retrieved for query {i+1} using bigram inverted index: {len(ans)}\")\n",
        "  print(f\"Names of documents retrieved for query {i+1} using bigram inverted index: {ans}\")\n",
        "  print(f\"Number of documents retrieved for query {i+1} using positional index: {len(pos_ans)}\")\n",
        "  print(f\"Names of documents retrieved for query {i+1} using positional index: {sorted(list(pos_ans.keys()))}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "7065473ddf06590dfe845d506c9721c9c7a6baf42eb3fc876ce07c8f7fc6e906"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
